{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.random as four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████| 118287/118287 [23.6m elapsed, 0s remaining, 81.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "#insert the directory of COCO dataset\n",
    "labels_file = r\"C:\\Users\\Danny\\Downloads\\Compressed\\coco2017\\annotations\\person_keypoints_train2017.json\"\n",
    "dataset_file = r\"C:\\Users\\Danny\\Downloads\\Compressed\\coco2017\\train2017\" #folder\n",
    "dataset_train = fo.Dataset.from_dir(\n",
    "    dataset_type = fo.types.COCODetectionDataset,\n",
    "    dataset_dir = dataset_file,\n",
    "    labels_path = labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 5000/5000 [57.7s elapsed, 0s remaining, 89.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "#insert the directory of COCO dataset\n",
    "labels_file = r\"C:\\Users\\Danny\\Downloads\\Compressed\\coco2017\\annotations\\person_keypoints_val2017.json\"\n",
    "dataset_file = r\"C:\\Users\\Danny\\Downloads\\Compressed\\coco2017\\val2017\" #folder\n",
    "dataset_valid = fo.Dataset.from_dir(\n",
    "    dataset_type = fo.types.COCODetectionDataset,\n",
    "    dataset_dir = dataset_file,\n",
    "    labels_path = labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0338\n",
      "0.05\n",
      "0.0063\n"
     ]
    }
   ],
   "source": [
    "needed_amount_train = round((1000/118288)*1, 4)\n",
    "needed_amount_valid = round((100/5000)*1, 4)\n",
    "needed_amount_test = round((200/118288)*1, 4)\n",
    "print(needed_amount_train)\n",
    "print(needed_amount_valid)\n",
    "print(needed_amount_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fiftyone.core.fields.EmbeddedDocumentField at 0x2bd98753fa0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.get_field(\"keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "four.random_split(\n",
    "    dataset_train, \n",
    "    {\"train\": needed_amount_train, \"test\": needed_amount_test, \"valid\": needed_amount_valid}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 766, 'unneeded': 113410, 'train': 4111}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.count_sample_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "four.random_split(\n",
    "    dataset_valid, \n",
    "    {\"needed\": needed_amount_valid, \"unneeded\": (1 - needed_amount_valid)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_train = dataset_train.match_tags(\"train\")\n",
    "view_valid = dataset_valid.match_tags(\"needed\")\n",
    "view_test = dataset_train.match_tags(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found multiple fields ['detections', 'segmentations', 'keypoints'] with compatible type (<class 'fiftyone.core.labels.Detections'>, <class 'fiftyone.core.labels.Polylines'>, <class 'fiftyone.core.labels.Keypoints'>); exporting 'detections'\n",
      " 100% |███████████████| 4111/4111 [50.5s elapsed, 0s remaining, 91.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "view_train.export(export_dir=r'dataset_train', \n",
    "            labels_path=r\"coco2_train.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found multiple fields ['detections', 'segmentations', 'keypoints'] with compatible type (<class 'fiftyone.core.labels.Detections'>, <class 'fiftyone.core.labels.Polylines'>, <class 'fiftyone.core.labels.Keypoints'>); exporting 'detections'\n",
      " 100% |█████████████████| 250/250 [1.7s elapsed, 0s remaining, 156.0 samples/s]         \n"
     ]
    }
   ],
   "source": [
    "view_valid.export(export_dir=r'dataset_valid', \n",
    "            labels_path=r\"coco2_valid.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found multiple fields ['detections', 'segmentations', 'keypoints'] with compatible type (<class 'fiftyone.core.labels.Detections'>, <class 'fiftyone.core.labels.Polylines'>, <class 'fiftyone.core.labels.Keypoints'>); exporting 'detections'\n",
      " 100% |█████████████████| 766/766 [8.9s elapsed, 0s remaining, 85.2 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "view_test.export(export_dir=r'dataset_test', \n",
    "            labels_path=r\"coco2_test.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_train\\\\detections_coco2_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Master Data Science\\Research Project 1\\Shrinking_COCO_Dataset\\keypointv2.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master%20Data%20Science/Research%20Project%201/Shrinking_COCO_Dataset/keypointv2.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Master%20Data%20Science/Research%20Project%201/Shrinking_COCO_Dataset/keypointv2.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset_train\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdetections_coco2_train.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m detections_file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master%20Data%20Science/Research%20Project%201/Shrinking_COCO_Dataset/keypointv2.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     detections_train \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(detections_file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master%20Data%20Science/Research%20Project%201/Shrinking_COCO_Dataset/keypointv2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdataset_train\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mkeypoints_coco2_train.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m keypoints_file:\n",
      "File \u001b[1;32mc:\\Users\\Danny\\mambaforge-pypy3\\envs\\fiftyone\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset_train\\\\detections_coco2_train.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(r\"dataset_train\\detections_coco2_train.json\", 'r') as detections_file:\n",
    "    detections_train = json.load(detections_file)\n",
    "with open(r\"dataset_train\\keypoints_coco2_train.json\", 'r') as keypoints_file:\n",
    "    keypoints_train = json.load(keypoints_file)\n",
    "with open(r\"dataset_train\\segmentations_coco2_train.json\", 'r') as segmentations_file:\n",
    "    segmentations_train = json.load(segmentations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"dataset_valid\\detections_coco2_valid.json\", 'r') as detections_file:\n",
    "    detections_valid = json.load(detections_file)\n",
    "with open(r\"dataset_valid\\keypoints_coco2_valid.json\", 'r') as keypoints_file:\n",
    "    keypoints_valid = json.load(keypoints_file)\n",
    "with open(r\"dataset_valid\\segmentations_coco2_valid.json\", 'r') as segmentations_file:\n",
    "    segmentations_valid = json.load(segmentations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"dataset_test\\detections_coco2_test.json\", 'r') as detections_file:\n",
    "    detections_test = json.load(detections_file)\n",
    "with open(r\"dataset_test\\keypoints_coco2_test.json\", 'r') as keypoints_file:\n",
    "    keypoints_test = json.load(keypoints_file)\n",
    "with open(r\"dataset_test\\segmentations_coco2_test.json\", 'r') as segmentations_file:\n",
    "    segmentations_test = json.load(segmentations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(detections_train[\"annotations\"])):\n",
    "    # Merge the annotations of the two dictionaries\n",
    "    detections_train[\"annotations\"][i].update(keypoints_train[\"annotations\"][i])\n",
    "    detections_train[\"annotations\"][i].update(segmentations_train[\"annotations\"][i])\n",
    "\n",
    "# Convert the merged dictionary back into a JSON string\n",
    "detections_keypoints_segmentations_train = json.dumps(detections_train, indent=4)\n",
    "\n",
    "with open('detections_keypoints_segmentations_train.json', 'w') as merged_file:\n",
    "    merged_file.write(detections_keypoints_segmentations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(detections_valid[\"annotations\"])):\n",
    "    # Merge the annotations of the two dictionaries\n",
    "    detections_valid[\"annotations\"][i].update(keypoints_valid[\"annotations\"][i])\n",
    "    detections_valid[\"annotations\"][i].update(segmentations_valid[\"annotations\"][i])\n",
    "\n",
    "# Convert the merged dictionary back into a JSON string\n",
    "detections_keypoints_segmentations_valid = json.dumps(detections_valid, indent=4)\n",
    "\n",
    "with open('detections_keypoints_segmentations_valid.json', 'w') as merged_file:\n",
    "    merged_file.write(detections_keypoints_segmentations_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(detections_test[\"annotations\"])):\n",
    "    # Merge the annotations of the two dictionaries\n",
    "    detections_test[\"annotations\"][i].update(keypoints_test[\"annotations\"][i])\n",
    "    detections_test[\"annotations\"][i].update(segmentations_test[\"annotations\"][i])\n",
    "\n",
    "# Convert the merged dictionary back into a JSON string\n",
    "detections_keypoints_segmentations_test = json.dumps(detections_test, indent=4)\n",
    "\n",
    "with open('detections_keypoints_segmentations_test.json', 'w') as merged_file:\n",
    "    merged_file.write(detections_keypoints_segmentations_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
