{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.random as four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████| 118287/118287 [18.0m elapsed, 0s remaining, 119.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "#insert the directory of COCO dataset\n",
    "labels_file = r\"C:\\Users\\Danny\\Downloads\\Compressed\\coco2017\\annotations\\person_keypoints_train2017.json\"\n",
    "dataset_file = r\"C:\\Users\\Danny\\Downloads\\Compressed\\coco2017\\train2017\" #folder\n",
    "dataset_train = fo.Dataset.from_dir(\n",
    "    dataset_type = fo.types.COCODetectionDataset,\n",
    "    dataset_dir = dataset_file,\n",
    "    labels_path = labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 5000/5000 [41.6s elapsed, 0s remaining, 126.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "#insert the directory of COCO dataset\n",
    "labels_file = r\"C:\\Users\\Danny\\Downloads\\Compressed\\coco2017\\annotations\\person_keypoints_val2017.json\"\n",
    "dataset_file = r\"C:\\Users\\Danny\\Downloads\\Compressed\\coco2017\\val2017\" #folder\n",
    "dataset_valid = fo.Dataset.from_dir(\n",
    "    dataset_type = fo.types.COCODetectionDataset,\n",
    "    dataset_dir = dataset_file,\n",
    "    labels_path = labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0339\n",
      "0.05\n",
      "0.0064\n"
     ]
    }
   ],
   "source": [
    "needed_amount_train = round((4010/118288)*1, 4)\n",
    "needed_amount_valid = round((250/5000)*1, 4)\n",
    "needed_amount_test = round((760/118288)*1, 4)\n",
    "print(needed_amount_train)\n",
    "print(needed_amount_valid)\n",
    "print(needed_amount_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "four.random_split(\n",
    "    dataset_train, \n",
    "    {\"train\": needed_amount_train, \"test\": needed_amount_test, \"unneeded\": (1 - needed_amount_train - needed_amount_test)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 4010, 'unneeded': 113520, 'test': 757}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.count_sample_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "four.random_split(\n",
    "    dataset_valid, \n",
    "    {\"needed\": needed_amount_valid, \"unneeded\": (1 - needed_amount_valid)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_train_detections = dataset_train.match_tags(\"train\").select_fields('detections')\n",
    "view_train_segmentations = dataset_train.match_tags(\"train\").select_fields('segmentations')\n",
    "view_train_keypoints = dataset_train.match_tags(\"train\").select_fields('keypoints')\n",
    "\n",
    "view_valid_detections = dataset_valid.match_tags(\"needed\").select_fields('detections')\n",
    "view_valid_segmentations = dataset_valid.match_tags(\"needed\").select_fields('segmentations')\n",
    "view_valid_keypoints = dataset_valid.match_tags(\"needed\").select_fields('keypoints')\n",
    "\n",
    "view_test_detections = dataset_train.match_tags(\"test\").select_fields('detections')\n",
    "view_test_segmentations = dataset_train.match_tags(\"test\").select_fields('segmentations')\n",
    "view_test_keypoints = dataset_train.match_tags(\"test\").select_fields('keypoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||--------------|    0/4010 [5.3ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 4010/4010 [21.2s elapsed, 0s remaining, 181.1 samples/s]      \n",
      "Directory 'dataset_train' already exists; export will be merged with existing files\n",
      " 100% |███████████████| 4010/4010 [1.0m elapsed, 0s remaining, 61.1 samples/s]       \n",
      "Directory 'dataset_train' already exists; export will be merged with existing files\n",
      " 100% |███████████████| 4010/4010 [9.6s elapsed, 0s remaining, 463.4 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "view_train_detections.export(export_dir=r'dataset_train', \n",
    "            labels_path=r\"detections_coco2_train.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)\n",
    "\n",
    "view_train_segmentations.export(export_dir=r'dataset_train', \n",
    "            labels_path=r\"segmentations_coco2_train.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)\n",
    "\n",
    "view_train_keypoints.export(export_dir=r'dataset_train', \n",
    "            labels_path=r\"keypoints_coco2_train.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||----------------|   0/250 [4.8ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 250/250 [537.4ms elapsed, 0s remaining, 465.2 samples/s]      \n",
      "Directory 'dataset_valid' already exists; export will be merged with existing files\n",
      " 100% |█████████████████| 250/250 [3.9s elapsed, 0s remaining, 85.5 samples/s]      \n",
      "Directory 'dataset_valid' already exists; export will be merged with existing files\n",
      " 100% |█████████████████| 250/250 [568.3ms elapsed, 0s remaining, 439.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "view_valid_detections.export(export_dir=r'dataset_valid', \n",
    "            labels_path=r\"detections_coco2_valid.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)\n",
    "\n",
    "view_valid_segmentations.export(export_dir=r'dataset_valid', \n",
    "            labels_path=r\"segmentations_coco2_valid.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)\n",
    "\n",
    "view_valid_keypoints.export(export_dir=r'dataset_valid', \n",
    "            labels_path=r\"keypoints_coco2_valid.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 757/757 [4.0s elapsed, 0s remaining, 191.6 samples/s]      \n",
      "Directory 'dataset_test' already exists; export will be merged with existing files\n",
      " 100% |█████████████████| 757/757 [11.6s elapsed, 0s remaining, 45.9 samples/s]      \n",
      "Directory 'dataset_test' already exists; export will be merged with existing files\n",
      " 100% |█████████████████| 757/757 [1.9s elapsed, 0s remaining, 436.0 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "view_test_detections.export(export_dir=r'dataset_test', \n",
    "            labels_path=r\"detections_coco2_test.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)\n",
    "\n",
    "view_test_segmentations.export(export_dir=r'dataset_test', \n",
    "            labels_path=r\"segmentations_coco2_test.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)\n",
    "\n",
    "view_test_keypoints.export(export_dir=r'dataset_test', \n",
    "            labels_path=r\"keypoints_coco2_test.json\",\n",
    "            dataset_type=fo.types.COCODetectionDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"dataset_train\\detections_coco2_train.json\", 'r') as detections_file:\n",
    "    detections_train = json.load(detections_file)\n",
    "with open(r\"dataset_train\\keypoints_coco2_train.json\", 'r') as keypoints_file:\n",
    "    keypoints_train = json.load(keypoints_file)\n",
    "with open(r\"dataset_train\\segmentations_coco2_train.json\", 'r') as segmentations_file:\n",
    "    segmentations_train = json.load(segmentations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"dataset_valid\\detections_coco2_valid.json\", 'r') as detections_file:\n",
    "    detections_valid = json.load(detections_file)\n",
    "with open(r\"dataset_valid\\keypoints_coco2_valid.json\", 'r') as keypoints_file:\n",
    "    keypoints_valid = json.load(keypoints_file)\n",
    "with open(r\"dataset_valid\\segmentations_coco2_valid.json\", 'r') as segmentations_file:\n",
    "    segmentations_valid = json.load(segmentations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"dataset_test\\detections_coco2_test.json\", 'r') as detections_file:\n",
    "    detections_test = json.load(detections_file)\n",
    "with open(r\"dataset_test\\keypoints_coco2_test.json\", 'r') as keypoints_file:\n",
    "    keypoints_test = json.load(keypoints_file)\n",
    "with open(r\"dataset_test\\segmentations_coco2_test.json\", 'r') as segmentations_file:\n",
    "    segmentations_test = json.load(segmentations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(detections_train[\"annotations\"])):\n",
    "    # Merge the annotations of the two dictionaries\n",
    "    detections_train[\"annotations\"][i].update(keypoints_train[\"annotations\"][i])\n",
    "    detections_train[\"annotations\"][i].update(segmentations_train[\"annotations\"][i])\n",
    "\n",
    "# Convert the merged dictionary back into a JSON string\n",
    "detections_keypoints_segmentations_train = json.dumps(detections_train, indent=4)\n",
    "\n",
    "with open('detections_keypoints_segmentations_train.json', 'w') as merged_file:\n",
    "    merged_file.write(detections_keypoints_segmentations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(detections_valid[\"annotations\"])):\n",
    "    # Merge the annotations of the two dictionaries\n",
    "    detections_valid[\"annotations\"][i].update(keypoints_valid[\"annotations\"][i])\n",
    "    detections_valid[\"annotations\"][i].update(segmentations_valid[\"annotations\"][i])\n",
    "\n",
    "# Convert the merged dictionary back into a JSON string\n",
    "detections_keypoints_segmentations_valid = json.dumps(detections_valid, indent=4)\n",
    "\n",
    "with open('detections_keypoints_segmentations_valid.json', 'w') as merged_file:\n",
    "    merged_file.write(detections_keypoints_segmentations_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(detections_test[\"annotations\"])):\n",
    "    # Merge the annotations of the two dictionaries\n",
    "    detections_test[\"annotations\"][i].update(keypoints_test[\"annotations\"][i])\n",
    "    detections_test[\"annotations\"][i].update(segmentations_test[\"annotations\"][i])\n",
    "\n",
    "# Convert the merged dictionary back into a JSON string\n",
    "detections_keypoints_segmentations_test = json.dumps(detections_test, indent=4)\n",
    "\n",
    "with open('detections_keypoints_segmentations_test.json', 'w') as merged_file:\n",
    "    merged_file.write(detections_keypoints_segmentations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
